import time
import os
from basereal import BaseReal
from logger import logger

def llm_response(message, nerfreal: BaseReal):
    """
    LLMå“åº”å‡½æ•°ï¼Œæ”¯æŒå¤šç§LLMæä¾›å•†
    æ”¯æŒçš„æä¾›å•†ï¼šdashscopeï¼ˆé˜¿é‡Œäº‘ï¼‰ã€ollamaï¼ˆæœ¬åœ°ï¼‰
    """
    start = time.perf_counter()
    logger.info(f"ğŸ¤– === LLMå“åº”å¼€å§‹ ===")
    logger.info(f"ğŸ’¬ ç”¨æˆ·æ¶ˆæ¯: '{message[:50]}{'...' if len(message) > 50 else ''}'")

    # è·å–LLMé…ç½®
    llm_provider = getattr(nerfreal.opt, 'llm_provider', 'dashscope')
    logger.info(f"ğŸ”§ LLMæä¾›å•†: {llm_provider}")

    if llm_provider == 'ollama':
        logger.info("ğŸš€ ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹")
        _ollama_response(message, nerfreal, start)
    else:
        logger.info("â˜ï¸ ä½¿ç”¨é˜¿é‡Œäº‘DashScope")
        _dashscope_response(message, nerfreal, start)

def _dashscope_response(message, nerfreal: BaseReal, start_time):
    """é˜¿é‡Œäº‘DashScopeå“åº”å¤„ç†"""
    from openai import OpenAI

    client = OpenAI(
        # å¦‚æœæ‚¨æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·åœ¨æ­¤å¤„ç”¨æ‚¨çš„API Keyè¿›è¡Œæ›¿æ¢
        api_key=os.getenv("DASHSCOPE_API_KEY"),
        # å¡«å†™DashScope SDKçš„base_url
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    )
    end = time.perf_counter()
    logger.info(f"llm Time init (dashscope): {end-start_time}s")

    model = getattr(nerfreal.opt, 'llm_model', 'qwen-plus')
    system_prompt = getattr(nerfreal.opt, 'llm_system_prompt', 'You are a helpful assistant.')

    completion = client.chat.completions.create(
        model=model,
        messages=[{'role': 'system', 'content': system_prompt},
                  {'role': 'user', 'content': message}],
        stream=True,
        # é€šè¿‡ä»¥ä¸‹è®¾ç½®ï¼Œåœ¨æµå¼è¾“å‡ºçš„æœ€åä¸€è¡Œå±•ç¤ºtokenä½¿ç”¨ä¿¡æ¯
        stream_options={"include_usage": True}
    )

    _process_stream_response(completion, nerfreal, start_time)

def _ollama_response(message, nerfreal: BaseReal, start_time):
    """Ollamaæœ¬åœ°æ¨¡å‹å“åº”å¤„ç†"""
    try:
        import ollama
    except ImportError:
        logger.error("ollama package not found. Please install it with: pip install ollama")
        nerfreal.put_msg_txt("æŠ±æ­‰ï¼ŒOllamaæœåŠ¡ä¸å¯ç”¨ã€‚")
        return

    # è·å–Ollamaé…ç½®
    ollama_host = getattr(nerfreal.opt, 'ollama_host', 'http://localhost:11434')
    model = getattr(nerfreal.opt, 'llm_model', 'llama3.2')
    system_prompt = getattr(nerfreal.opt, 'llm_system_prompt', 'You are a helpful assistant.')

    try:
        # åˆ›å»ºOllamaå®¢æˆ·ç«¯
        client = ollama.Client(host=ollama_host)

        end = time.perf_counter()
        logger.info(f"llm Time init (ollama): {end-start_time}s")

        # å‘é€è¯·æ±‚åˆ°Ollama
        response = client.chat(
            model=model,
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': message}
            ],
            stream=True
        )

        _process_ollama_stream_response(response, nerfreal, start_time)

    except Exception as e:
        logger.error(f"Ollama request failed: {e}")
        nerfreal.put_msg_txt("æŠ±æ­‰ï¼Œæ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ã€‚")

def _process_stream_response(completion, nerfreal: BaseReal, start_time):
    """å¤„ç†OpenAIå…¼å®¹çš„æµå¼å“åº”"""
    result = ""
    first = True

    for chunk in completion:
        if len(chunk.choices) > 0:
            if first:
                end = time.perf_counter()
                logger.info(f"llm Time to first chunk: {end-start_time}s")
                first = False

            msg = chunk.choices[0].delta.content
            if msg:
                result = _process_message_chunk(msg, result, nerfreal)

    end = time.perf_counter()
    logger.info(f"â±ï¸ LLMå“åº”æ€»æ—¶é—´: {end-start_time:.2f}s")
    if result:
        logger.info(f"ğŸ¤ å‘é€æœ€ç»ˆTTSæ–‡æœ¬: '{result}'")
        nerfreal.put_msg_txt(result)
    logger.info("âœ… === LLMå“åº”å®Œæˆ ===")

def _process_ollama_stream_response(response, nerfreal: BaseReal, start_time):
    """å¤„ç†Ollamaçš„æµå¼å“åº”"""
    result = ""
    first = True

    for chunk in response:
        if first:
            end = time.perf_counter()
            logger.info(f"llm Time to first chunk: {end-start_time}s")
            first = False

        msg = chunk.get('message', {}).get('content', '')
        if msg:
            result = _process_message_chunk(msg, result, nerfreal)

    end = time.perf_counter()
    logger.info(f"â±ï¸ LLMå“åº”æ€»æ—¶é—´: {end-start_time:.2f}s")
    if result:
        logger.info(f"ğŸ¤ å‘é€æœ€ç»ˆTTSæ–‡æœ¬: '{result}'")
        nerfreal.put_msg_txt(result)
    logger.info("âœ… === LLMå“åº”å®Œæˆ ===")

def _process_message_chunk(msg, result, nerfreal: BaseReal):
    """å¤„ç†æ¶ˆæ¯å—ï¼ŒæŒ‰æ ‡ç‚¹ç¬¦å·åˆ†æ®µ"""
    lastpos = 0

    for i, char in enumerate(msg):
        if char in ",.!;:ï¼Œã€‚ï¼ï¼Ÿï¼šï¼›":
            result = result + msg[lastpos:i+1]
            lastpos = i+1
            if len(result) > 10:
                logger.info(f"ğŸ¤ å‘é€TTSæ–‡æœ¬: '{result}'")
                nerfreal.put_msg_txt(result)
                result = ""

    result = result + msg[lastpos:]
    return result