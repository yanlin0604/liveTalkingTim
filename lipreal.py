###############################################################################
#  Copyright (C) 2025 unimed
#  email: zengyanlin99@gmail.com
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
###############################################################################

import math
import torch
import numpy as np

# from .utils import *
import os
import time
import cv2
import glob
import pickle
import copy

import queue
from queue import Queue
from threading import Thread, Event
import torch.multiprocessing as mp


from lipasr import LipASR
import asyncio
from av import AudioFrame, VideoFrame

# Wav2Lip æ¨¡åž‹å°†åœ¨ load_model å‡½æ•°ä¸­åŠ¨æ€å¯¼å…¥
from basereal import BaseReal

# from imgcache import ImgCache

from tqdm import tqdm
from logger import logger

device = (
    "cuda"
    if torch.cuda.is_available()
    else (
        "mps"
        if (hasattr(torch.backends, "mps") and torch.backends.mps.is_available())
        else "cpu"
    )
)
print("Using {} for inference.".format(device))


def _load(checkpoint_path):
    if device == "cuda":
        checkpoint = torch.load(checkpoint_path)  # ,weights_only=True
    else:
        checkpoint = torch.load(
            checkpoint_path, map_location=lambda storage, loc: storage
        )
    return checkpoint


def detect_model_version(checkpoint_state_dict):
    """
    æ£€æµ‹æ¨¡åž‹ç‰ˆæœ¬ï¼šé€šè¿‡æ£€æŸ¥ç‰¹å®šå±‚çš„å­˜åœ¨å’Œå½¢çŠ¶æ¥åˆ¤æ–­æ¨¡åž‹æž¶æž„
    """
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¬¬7ä¸ªface_encoder_blocksï¼ˆ384é«˜åˆ†è¾¨çŽ‡ç‰ˆæœ¬ç‰¹æœ‰ï¼‰
    has_encoder_7 = any(
        "face_encoder_blocks.7." in key for key in checkpoint_state_dict.keys()
    )

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨face_decoder_blocks.2.2ï¼ˆ384ç‰ˆæœ¬ç‰¹æœ‰çš„é¢å¤–å±‚ï¼‰
    has_decoder_2_2 = any(
        "face_decoder_blocks.2.2." in key for key in checkpoint_state_dict.keys()
    )

    # æ£€æŸ¥face_encoder_blocks.6.1çš„å·ç§¯æ ¸å¤§å°
    encoder_6_1_key = "face_encoder_blocks.6.1.conv_block.0.weight"
    if encoder_6_1_key in checkpoint_state_dict:
        kernel_shape = checkpoint_state_dict[encoder_6_1_key].shape
        logger.info(f"face_encoder_blocks.6.1 kernel shape: {kernel_shape}")
        if len(kernel_shape) >= 3 and kernel_shape[2] == 1:  # 1x1å·ç§¯ -> 384ç‰ˆæœ¬
            if has_encoder_7:
                return "384_full"  # å®Œæ•´çš„384ç‰ˆæœ¬ï¼ˆ8å±‚ï¼‰
            else:
                return "384_original"  # åŽŸå§‹384ç‰ˆæœ¬ï¼ˆ7å±‚ï¼‰

    # æ£€æŸ¥face_decoder_blocks.3çš„é€šé“æ•°
    decoder_3_key = "face_decoder_blocks.3.0.conv_block.0.weight"
    if decoder_3_key in checkpoint_state_dict:
        decoder_3_shape = checkpoint_state_dict[decoder_3_key].shape
        logger.info(f"face_decoder_blocks.3 shape: {decoder_3_shape}")
        if decoder_3_shape[0] == 768 and decoder_3_shape[1] == 384:  # 384ç‰ˆæœ¬ç‰¹å¾
            return "384_original"
        elif decoder_3_shape[0] == 1024 and decoder_3_shape[1] == 512:  # 256ç‰ˆæœ¬ç‰¹å¾
            return "256"

    # é»˜è®¤è¿”å›ž256ç‰ˆæœ¬ï¼ˆå‘åŽå…¼å®¹ï¼‰
    logger.warning("æ— æ³•ç¡®å®šæ¨¡åž‹ç‰ˆæœ¬ï¼Œé»˜è®¤ä½¿ç”¨256ç‰ˆæœ¬")
    return "256"


def create_compatible_model(checkpoint_state_dict):
    """
    æ ¹æ®æ£€æŸ¥ç‚¹åˆ›å»ºå…¼å®¹çš„æ¨¡åž‹æž¶æž„
    """
    try:
        # åˆ†æžæ£€æŸ¥ç‚¹ä¸­çš„å±‚ç»“æž„
        encoder_layers = set()
        decoder_layers = set()

        for key in checkpoint_state_dict.keys():
            if "face_encoder_blocks." in key:
                layer_num = int(key.split(".")[1])
                encoder_layers.add(layer_num)
            elif "face_decoder_blocks." in key:
                layer_num = int(key.split(".")[1])
                decoder_layers.add(layer_num)

        logger.info(f"æ£€æŸ¥ç‚¹ä¸­çš„ç¼–ç å™¨å±‚: {sorted(encoder_layers)}")
        logger.info(f"æ£€æŸ¥ç‚¹ä¸­çš„è§£ç å™¨å±‚: {sorted(decoder_layers)}")

        # å¦‚æžœæœ‰8å±‚ç¼–ç å™¨ï¼Œå°è¯•ä½¿ç”¨384ç‰ˆæœ¬ä½†è·³è¿‡ä¸åŒ¹é…çš„å±‚
        if 7 in encoder_layers:
            logger.info("æ£€æµ‹åˆ°8å±‚ç¼–ç å™¨ï¼Œä½¿ç”¨384ç‰ˆæœ¬æ¨¡åž‹")
            from wav2lip.models.wav2lip import Wav2Lip as Wav2Lip384

            model = Wav2Lip384()

            # å°è¯•éƒ¨åˆ†åŠ è½½
            model_dict = model.state_dict()
            compatible_dict = {}

            for k, v in checkpoint_state_dict.items():
                if k in model_dict and model_dict[k].shape == v.shape:
                    compatible_dict[k] = v
                else:
                    logger.debug(f"è·³è¿‡ä¸å…¼å®¹çš„å±‚: {k}")

            model_dict.update(compatible_dict)
            model.load_state_dict(model_dict)
            logger.info(f"æˆåŠŸåŠ è½½ {len(compatible_dict)} ä¸ªå…¼å®¹å±‚")
            return model

        return None

    except Exception as e:
        logger.error(f"åˆ›å»ºå…¼å®¹æ¨¡åž‹å¤±è´¥: {e}")
        return None


def load_model(path):
    logger.info("Load checkpoint from: {}".format(path))
    checkpoint = _load(path)
    s = checkpoint["state_dict"]
    new_s = {}
    for k, v in s.items():
        new_s[k.replace("module.", "")] = v

    # æ£€æµ‹æ¨¡åž‹ç‰ˆæœ¬
    model_version = detect_model_version(new_s)
    logger.info(f"æ£€æµ‹åˆ°æ¨¡åž‹ç‰ˆæœ¬: {model_version}")

    # å°è¯•åŠ è½½ç­–ç•¥åˆ—è¡¨
    strategies = []

    if "384" in model_version:
        strategies = [
            ("384ç‰ˆæœ¬", "wav2lip.models.wav2lip", "Wav2Lip"),
            ("256ç‰ˆæœ¬", "wav2lip.models.wav2lip_v2", "Wav2Lip"),
        ]
    else:
        strategies = [
            ("256ç‰ˆæœ¬", "wav2lip.models.wav2lip_v2", "Wav2Lip"),
            ("384ç‰ˆæœ¬", "wav2lip.models.wav2lip", "Wav2Lip"),
        ]

    # å°è¯•æ¯ç§ç­–ç•¥
    for strategy_name, module_name, class_name in strategies:
        try:
            logger.info(f"å°è¯•ä½¿ç”¨{strategy_name}æ¨¡åž‹...")
            module = __import__(module_name, fromlist=[class_name])
            ModelClass = getattr(module, class_name)
            model = ModelClass()

            # å°è¯•åŠ è½½ï¼Œå…ˆä¸¥æ ¼æ¨¡å¼ï¼Œå†å®½æ¾æ¨¡å¼
            try:
                model.load_state_dict(new_s, strict=True)
                logger.info(f"âœ… {strategy_name}æ¨¡åž‹åŠ è½½æˆåŠŸï¼ˆä¸¥æ ¼æ¨¡å¼ï¼‰")
                break
            except Exception:
                model.load_state_dict(new_s, strict=False)
                logger.info(f"âœ… {strategy_name}æ¨¡åž‹åŠ è½½æˆåŠŸï¼ˆå…¼å®¹æ¨¡å¼ï¼‰")
                break

        except Exception as e:
            logger.warning(f"âŒ {strategy_name}æ¨¡åž‹åŠ è½½å¤±è´¥: {str(e)[:100]}...")
            continue
    else:
        # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œç»™å‡ºæ˜Žç¡®çš„è§£å†³å»ºè®®
        logger.error("ðŸš¨ æ‰€æœ‰æ¨¡åž‹åŠ è½½ç­–ç•¥éƒ½å¤±è´¥äº†ï¼")
        logger.error("ðŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆï¼š")
        logger.error("1. ä¸‹è½½å®˜æ–¹ wav2lip256.pth æ¨¡åž‹")
        logger.error("2. é‡å‘½åä¸º wav2lip.pth å¹¶æ”¾åœ¨ ./models/ ç›®å½•")
        logger.error("3. ä¸‹è½½åœ°å€ï¼šhttps://pan.quark.cn/s/83a750323ef0")
        raise RuntimeError("æ— æ³•åŠ è½½ä»»ä½•å…¼å®¹çš„æ¨¡åž‹æž¶æž„ï¼Œè¯·ä½¿ç”¨å®˜æ–¹æŽ¨èçš„æ¨¡åž‹æ–‡ä»¶")

    model = model.to(device)
    return model.eval()


def load_avatar(avatar_id):
    avatar_path = f"./data/avatars/{avatar_id}"
    full_imgs_path = f"{avatar_path}/full_imgs"
    face_imgs_path = f"{avatar_path}/face_imgs"
    coords_path = f"{avatar_path}/coords.pkl"

    with open(coords_path, "rb") as f:
        coord_list_cycle = pickle.load(f)
    input_img_list = glob.glob(os.path.join(full_imgs_path, "*.[jpJP][pnPN]*[gG]"))
    input_img_list = sorted(
        input_img_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0])
    )
    frame_list_cycle = read_imgs(input_img_list)
    # self.imagecache = ImgCache(len(self.coord_list_cycle),self.full_imgs_path,1000)
    input_face_list = glob.glob(os.path.join(face_imgs_path, "*.[jpJP][pnPN]*[gG]"))
    input_face_list = sorted(
        input_face_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0])
    )
    face_list_cycle = read_imgs(input_face_list)

    return frame_list_cycle, face_list_cycle, coord_list_cycle


@torch.no_grad()
def warm_up(batch_size, model, modelres):
    # å¢žå¼ºé¢„çƒ­å‡½æ•° - å¤šè½®é¢„çƒ­ç¡®ä¿ç¨³å®šæ€§
    logger.info("warmup model...")

    # è¿›è¡Œå¤šè½®é¢„çƒ­ï¼Œæ¨¡æ‹ŸçœŸå®žä½¿ç”¨åœºæ™¯
    for i in range(5):  # å¢žåŠ é¢„çƒ­è½®æ•°
        logger.info(f"warmup round {i+1}/5...")

        # ä½¿ç”¨ä¸åŒçš„éšæœºæ•°æ®æ¨¡æ‹ŸçœŸå®žåœºæ™¯
        img_batch = torch.randn(batch_size, 6, modelres, modelres).to(device)
        mel_batch = torch.randn(batch_size, 1, 80, 16).to(device)

        # æ‰§è¡ŒæŽ¨ç†
        _ = model(mel_batch, img_batch)

        # æ¸…ç†GPUç¼“å­˜ï¼Œå¼ºåˆ¶å†…å­˜é‡æ–°åˆ†é…
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    logger.info("warmup completed - model should be stable now")


def read_imgs(img_list):
    frames = []
    logger.info("reading images...")
    for img_path in tqdm(img_list):
        frame = cv2.imread(img_path)
        frames.append(frame)
    return frames


def __mirror_index(size, index):
    # size = len(self.coord_list_cycle)
    turn = index // size
    res = index % size
    if turn % 2 == 0:
        return res
    else:
        return size - res - 1


def inference(
    quit_event,
    batch_size,
    face_list_cycle,
    audio_feat_queue,
    audio_out_queue,
    res_frame_queue,
    model,
):

    # model = load_model("./models/wav2lip.pth")
    # input_face_list = glob.glob(os.path.join(face_imgs_path, '*.[jpJP][pnPN]*[gG]'))
    # input_face_list = sorted(input_face_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))
    # face_list_cycle = read_imgs(input_face_list)

    # input_latent_list_cycle = torch.load(latents_out_path)
    length = len(face_list_cycle)
    index = 0
    count = 0
    counttime = 0
    logger.info("start inference")
    while not quit_event.is_set():
        starttime = time.perf_counter()
        mel_batch = []
        try:
            mel_batch = audio_feat_queue.get(block=True, timeout=1)
        except queue.Empty:
            continue

        is_all_silence = True
        audio_frames = []
        for _ in range(batch_size * 2):
            frame, type, eventpoint = audio_out_queue.get()
            audio_frames.append((frame, type, eventpoint))
            if type == 0:
                is_all_silence = False

        if is_all_silence:
            for i in range(batch_size):
                res_frame_queue.put(
                    (
                        None,
                        __mirror_index(length, index),
                        audio_frames[i * 2 : i * 2 + 2],
                    )
                )
                index = index + 1
        else:
            # print('infer=======')
            t = time.perf_counter()
            img_batch = []
            for i in range(batch_size):
                idx = __mirror_index(length, index + i)
                face = face_list_cycle[idx]
                img_batch.append(face)
            img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)

            img_masked = img_batch.copy()
            img_masked[:, face.shape[0] // 2 :] = 0

            img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.0
            mel_batch = np.reshape(
                mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1]
            )

            img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(
                device
            )
            mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(
                device
            )

            with torch.no_grad():
                pred = model(mel_batch, img_batch)
            pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.0

            counttime += time.perf_counter() - t
            count += batch_size
            # _totalframe += 1
            if count >= 100:
                logger.info(f"------actual avg infer fps:{count/counttime:.4f}")
                count = 0
                counttime = 0
            for i, res_frame in enumerate(pred):
                # self.__pushmedia(res_frame,loop,audio_track,video_track)
                res_frame_queue.put(
                    (
                        res_frame,
                        __mirror_index(length, index),
                        audio_frames[i * 2 : i * 2 + 2],
                    )
                )
                index = index + 1
            # print('total batch time:',time.perf_counter()-starttime)
    logger.info("lipreal inference processor stop")


class LipReal(BaseReal):
    @torch.no_grad()
    def __init__(self, opt, model, avatar):
        super().__init__(opt)
        # self.opt = opt # shared with the trainer's opt to support in-place modification of rendering parameters.
        # self.W = opt.W
        # self.H = opt.H

        self.fps = opt.fps  # 20 ms per frame

        self.batch_size = opt.batch_size
        self.idx = 0
        self.res_frame_queue = Queue(self.batch_size * 2)  # mp.Queue
        # self.__loadavatar()
        self.model = model
        self.frame_list_cycle, self.face_list_cycle, self.coord_list_cycle = avatar

        # åŠ è½½è’™ç‰ˆå›¾åƒ
        mask_path = os.path.join(".", "models", "mask.png")
        self.mask_img = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        if self.mask_img is None:
            raise ValueError("æ‰¾ä¸åˆ°è’™ç‰ˆå›¾åƒ: " + mask_path)

        # é¢œè‰²åŒ¹é…é…ç½®å‚æ•°
        self.enable_color_matching = getattr(opt, 'enable_color_matching', True)
        self.color_matching_strength = getattr(opt, 'color_matching_strength', 0.6)

        # æ·»åŠ æ¸å˜è¿‡æ¸¡ç›¸å…³å˜é‡
        self.last_generated_frame = None  # ä¿å­˜æœ€åŽä¸€å¸§ç”Ÿæˆçš„é¢éƒ¨
        self.transition_frames = 10  # è¿‡æ¸¡å¸§æ•°ï¼ˆçº¦0.4ç§’ï¼‰
        self.current_transition_frame = 0  # å½“å‰è¿‡æ¸¡å¸§è®¡æ•°
        self.is_transitioning = False  # æ˜¯å¦æ­£åœ¨è¿‡æ¸¡ä¸­

        self.asr = LipASR(opt, self)
        self.asr.warm_up()

        self.render_event = mp.Event()

    def __del__(self):
        logger.info(f"lipreal({self.sessionid}) delete")

    def paste_back_frame(self, pred_frame, idx: int):
        """å°†é¢„æµ‹çš„é¢éƒ¨å¸§ç²˜è´´å›žåŽŸå§‹å¸§ï¼Œå¦‚æžœé™éŸ³åˆ™ä¿ç•™æœ€åŽä¸€å¸§çš„å˜´åž‹"""
        try:
            bbox = self.coord_list_cycle[idx]
            combine_frame = copy.deepcopy(self.frame_list_cycle[idx])

            frame_to_apply = None

            if pred_frame is not None:
                # æœ‰æ–°çš„é¢„æµ‹å¸§ï¼Œæ›´æ–°æœ€åŽä¸€å¸§
                self.last_generated_frame = pred_frame.copy()
                frame_to_apply = pred_frame
                logger.debug(
                    f"lipreal: ä½¿ç”¨æ–°é¢„æµ‹å¸§ï¼Œå·²æ›´æ–°last_generated_frameï¼Œidx={idx}"
                )
            elif self.last_generated_frame is not None:
                # é™éŸ³ï¼Œä½†æœ‰åŽ†å²å¸§ï¼Œä½¿ç”¨åŽ†å²å¸§
                frame_to_apply = self.last_generated_frame
                logger.debug(f"lipreal: é™éŸ³çŠ¶æ€ï¼Œä½¿ç”¨ä¿ç•™çš„æœ€åŽä¸€å¸§å˜´åž‹ï¼Œidx={idx}")
            else:
                logger.debug(f"lipreal: é™éŸ³çŠ¶æ€ä¸”æ— åŽ†å²å¸§ï¼Œä½¿ç”¨åŽŸå§‹å¸§ï¼Œidx={idx}")

            if frame_to_apply is not None:
                # å¦‚æžœæœ‰éœ€è¦åº”ç”¨çš„å¸§ï¼Œåˆ™è¿›è¡Œèžåˆ
                y1, y2, x1, x2 = bbox
                res_frame = cv2.resize(
                    frame_to_apply.astype(np.uint8), (x2 - x1, y2 - y1)
                )

                if hasattr(self, "mask_img") and self.mask_img is not None:
                    combine_frame = self._apply_mask_blend(
                        combine_frame, res_frame, bbox
                    )
                else:
                    combine_frame[y1:y2, x1:x2] = res_frame

            # å¦‚æžœ frame_to_apply is None (æ—¢æ²¡æœ‰æ–°å¸§ä¹Ÿæ²¡æœ‰åŽ†å²å¸§)ï¼Œåˆ™ç›´æŽ¥è¿”å›žåŽŸå§‹å¸§
            return combine_frame

        except Exception as e:
            logger.error(f"paste_back_frame error: {e}")
            return copy.deepcopy(self.frame_list_cycle[idx])

    def _apply_mask_blend(self, combine_frame, res_frame, bbox):
        """åº”ç”¨è’™ç‰ˆèžåˆï¼ŒåŒ…å«é¢œè‰²åŒ¹é…"""
        y1, y2, x1, x2 = bbox

        # è°ƒæ•´è’™ç‰ˆå¤§å°åˆ°é¢éƒ¨åŒºåŸŸ
        resized_mask = cv2.resize(self.mask_img, (x2 - x1, y2 - y1))
        _, binary_mask = cv2.threshold(resized_mask, 127, 255, cv2.THRESH_BINARY)
        binary_mask = binary_mask.astype(np.uint8)

        # åˆ›å»ºè·ç¦»å˜æ¢ç”¨äºŽç¾½åŒ–æ•ˆæžœ
        dist = cv2.distanceTransform(binary_mask, cv2.DIST_L2, 5)
        feather_radius = 15  # ç¾½åŒ–åŠå¾„ï¼ŒæŽ§åˆ¶èžåˆè¾¹ç¼˜çš„æŸ”å’Œç¨‹åº¦
        alpha = np.clip(dist / feather_radius, 0, 1)
        alpha = cv2.GaussianBlur(alpha, (5, 5), 0)  # é«˜æ–¯æ¨¡ç³Šå¹³æ»‘è¾¹ç¼˜

        # èŽ·å–åŽŸå§‹é¢éƒ¨å’Œç”Ÿæˆé¢éƒ¨
        original_face = combine_frame[y1:y2, x1:x2].astype(np.float32)
        generated_face = res_frame.astype(np.float32)

        # åº”ç”¨é¢œè‰²åŒ¹é…ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
        if self.enable_color_matching:
            color_matched_face = self._apply_color_matching(generated_face, original_face, alpha)
        else:
            color_matched_face = generated_face

        # èžåˆå›¾åƒï¼šalphaèžåˆé¢œè‰²åŒ¹é…åŽçš„ç”Ÿæˆé¢éƒ¨å’ŒåŽŸå§‹é¢éƒ¨
        blended_face = (
            alpha[..., None] * color_matched_face + (1 - alpha[..., None]) * original_face
        ).astype(np.uint8)

        combine_frame[y1:y2, x1:x2] = blended_face
        return combine_frame

    def _apply_color_matching(self, generated_face, original_face, alpha):
        """åº”ç”¨é¢œè‰²åŒ¹é…ï¼Œä½¿ç”Ÿæˆçš„å˜´åž‹é¢œè‰²ä¸ŽåŽŸå§‹é¢éƒ¨æ›´åŒ¹é…"""
        try:
            # åˆ›å»ºå˜´å”‡åŒºåŸŸè’™ç‰ˆï¼ˆalphaå€¼è¾ƒé«˜çš„åŒºåŸŸï¼‰
            lip_mask = (alpha > 0.3).astype(np.float32)

            if np.sum(lip_mask) == 0:
                return generated_face

            # è®¡ç®—åŽŸå§‹é¢éƒ¨å’Œç”Ÿæˆé¢éƒ¨åœ¨å˜´å”‡åŒºåŸŸçš„å¹³å‡é¢œè‰²
            original_lip_color = self._get_average_color(original_face, lip_mask)
            generated_lip_color = self._get_average_color(generated_face, lip_mask)

            # è®¡ç®—é¢œè‰²å·®å¼‚
            color_diff = original_lip_color - generated_lip_color

            # åº”ç”¨é¢œè‰²æ ¡æ­£ï¼Œå¼ºåº¦æ ¹æ®alphaå€¼è°ƒæ•´
            color_matched_face = generated_face.copy()
            for c in range(3):  # BGRä¸‰ä¸ªé€šé“
                adjustment = color_diff[c] * alpha * self.color_matching_strength
                color_matched_face[:, :, c] = np.clip(
                    color_matched_face[:, :, c] + adjustment, 0, 255
                )

            return color_matched_face

        except Exception as e:
            logger.warning(f"é¢œè‰²åŒ¹é…å¤±è´¥ï¼Œä½¿ç”¨åŽŸå§‹ç”Ÿæˆé¢éƒ¨: {e}")
            return generated_face

    def _get_average_color(self, image, mask):
        """è®¡ç®—è’™ç‰ˆåŒºåŸŸçš„å¹³å‡é¢œè‰²"""
        masked_pixels = image * mask[..., None]
        total_weight = np.sum(mask)
        if total_weight == 0:
            return np.array([0, 0, 0])

        avg_color = np.sum(masked_pixels, axis=(0, 1)) / total_weight
        return avg_color

    def _apply_transition_blend(
        self, combine_frame, res_frame, bbox, transition_weight
    ):
        """åº”ç”¨è¿‡æ¸¡èžåˆ"""
        y1, y2, x1, x2 = bbox

        # è°ƒæ•´è’™ç‰ˆå¤§å°åˆ°é¢éƒ¨åŒºåŸŸ
        resized_mask = cv2.resize(self.mask_img, (x2 - x1, y2 - y1))
        _, binary_mask = cv2.threshold(resized_mask, 127, 255, cv2.THRESH_BINARY)
        binary_mask = binary_mask.astype(np.uint8)

        # åˆ›å»ºè·ç¦»å˜æ¢ç”¨äºŽç¾½åŒ–æ•ˆæžœ
        dist = cv2.distanceTransform(binary_mask, cv2.DIST_L2, 5)
        feather_radius = 15
        alpha = np.clip(dist / feather_radius, 0, 1)
        alpha = cv2.GaussianBlur(alpha, (5, 5), 0)

        # åº”ç”¨è¿‡æ¸¡æƒé‡åˆ°alphaé€šé“
        alpha = alpha * transition_weight

        # èžåˆå›¾åƒ
        original_face = combine_frame[y1:y2, x1:x2].astype(np.float32)
        generated_face = res_frame.astype(np.float32)
        blended_face = (
            alpha[..., None] * generated_face + (1 - alpha[..., None]) * original_face
        ).astype(np.uint8)

        combine_frame[y1:y2, x1:x2] = blended_face
        return combine_frame

    def render(self, quit_event, loop=None, audio_track=None, video_track=None):
        # if self.opt.asr:
        #     self.asr.warm_up()

        self.tts.render(quit_event)
        self.init_customindex()
        process_thread = Thread(
            target=self.process_frames,
            args=(quit_event, loop, audio_track, video_track),
        )
        process_thread.start()

        Thread(
            target=inference,
            args=(
                quit_event,
                self.batch_size,
                self.face_list_cycle,
                self.asr.feat_queue,
                self.asr.output_queue,
                self.res_frame_queue,
                self.model,
            ),
        ).start()  # mp.Process

        # self.render_event.set() #start infer process render
        count = 0
        totaltime = 0
        _starttime = time.perf_counter()
        # _totalframe=0
        while not quit_event.is_set():
            # update texture every frame
            # audio stream thread...
            t = time.perf_counter()
            self.asr.run_step()

            # if video_track._queue.qsize()>=2*self.opt.batch_size:
            #     print('sleep qsize=',video_track._queue.qsize())
            #     time.sleep(0.04*video_track._queue.qsize()*0.8)
            if video_track and video_track._queue.qsize() >= 5:
                logger.debug("[lipreal] sleep qsize=%d", video_track._queue.qsize())
                time.sleep(0.04 * video_track._queue.qsize() * 0.8)

            # delay = _starttime+_totalframe*0.04-time.perf_counter() #40ms
            # if delay > 0:
            #     time.sleep(delay)
        # self.render_event.clear() #end infer process render
        logger.info("lipreal thread stop")
